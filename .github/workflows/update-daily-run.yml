name: Scrape and Upload

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'  # Daily at midnight UTC, adjust as needed

jobs:
  scrape-upload:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18

      - name: Install Node.js dependencies
        run: npm install
      

      - name: Run scraper and upload Phantom output
        id: run-scraper
        env:
          ALL_CREDENTIALS: ${{ secrets.ALL_CREDENTIALS }}
        run: |
          node ./your-nodejs-folder/scrape-and-upload.js > output.json
          # The last line of output.json contains the JSON with csvUrl and jsonUrl
          tail -n 1 output.json > urls.json
        shell: bash

      - name: Parse URLs from scraper output
        id: parse-urls
        run: |
          CSV_URL=$(jq -r '.csvUrl' urls.json)
          JSON_URL=$(jq -r '.jsonUrl' urls.json)
          echo "CSV_URL=$CSV_URL" >> $GITHUB_ENV
          echo "JSON_URL=$JSON_URL" >> $GITHUB_ENV

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.10

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 requests python-dotenv

      - name: Run Python uploader script
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
          CSV_URL: ${{ env.CSV_URL }}
          JSON_URL: ${{ env.JSON_URL }}
        run: |
          python ./your-python-folder/upload_from_urls.py
