name: Scrape and Upload

# Trigger manually or daily at midnight UTC
on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * *'

jobs:
  scrape-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18

      - name: Install Node.js dependencies
        run: npm install

      - name: Run scraper and capture URLs
        id: run-scraper
        env:
          ALL_CREDENTIALS: ${{ secrets.ALL_CREDENTIALS }}
        run: |
          node scrape-and-upload.js > output.json
          tail -n 1 output.json > urls.json
        shell: bash

      - name: Parse URLs from scraper output
        id: parse-urls
        run: |
          CSV_URL=$(jq -r '.csvUrl' urls.json)
          JSON_URL=$(jq -r '.jsonUrl' urls.json)
          echo "CSV_URL=$CSV_URL" >> $GITHUB_ENV
          echo "JSON_URL=$JSON_URL" >> $GITHUB_ENV

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 requests python-dotenv

      - name: Run Python upload script
        env:
          AWS_ACCESS_KEY_ID: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_ACCESS_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
          AWS_REGION: ${{ secrets.AWS_REGION }}
          S3_BUCKET_NAME: ${{ secrets.S3_BUCKET_NAME }}
          CSV_URL: ${{ env.CSV_URL }}
          JSON_URL: ${{ env.JSON_URL }}
        run: python upload_from_urls.py
