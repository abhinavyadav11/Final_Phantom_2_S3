name: 2 Scrape and Upload

# Trigger manually or daily at midnight UTC
on:
  workflow_dispatch:
  schedule:
    - cron: '30 20 * * *'

jobs:
  scrape-upload:
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v3

      - name: Setup Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18

      - name: Install Node.js dependencies
        run: npm install

      - name: Run scraper and capture URLs
        id: run-scraper
        env:
          ALL_CREDENTIALS: ${{ secrets.LINKEDIN_SCRAPING }}
        run: |
          node scrape-and-upload.js > output.json
          tail -n 1 output.json > urls.json
        shell: bash

      - name: Parse URLs from scraper output
        id: parse-urls
        run: |
          CSV_URL=$(jq -r '.csvUrl' urls.json)
          JSON_URL=$(jq -r '.jsonUrl' urls.json)
          echo "CSV_URL=$CSV_URL" >> $GITHUB_ENV
          echo "JSON_URL=$JSON_URL" >> $GITHUB_ENV

      - name: Debug env vars
        run: |
          echo "CSV_URL=$CSV_URL"
          echo "JSON_URL=$JSON_URL"

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: 3.11

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 requests python-dotenv pandas 

      - name: Run Python upload script
        env:
          ALL_CREDENTIALS: ${{ secrets.LINKEDIN_SCRAPING }}
          CSV_URL: ${{ env.CSV_URL }}
          JSON_URL: ${{ env.JSON_URL }}
        run: |
          CREDS=$(echo "$ALL_CREDENTIALS" | jq -r '.')
          export AWS_ACCESS_KEY_ID=$(echo "$CREDS" | jq -r '.AWS_ACCESS_KEY_ID')
          export AWS_SECRET_ACCESS_KEY=$(echo "$CREDS" | jq -r '.AWS_SECRET_ACCESS_KEY')
          export AWS_REGION=$(echo "$CREDS" | jq -r '.AWS_REGION')
          export S3_BUCKET_NAME=$(echo "$CREDS" | jq -r '.S3_BUCKET_NAME')
      
          python new_upload.py
