name: Run PhantomBuster and Upload to S3

on:
  workflow_dispatch:
  schedule:
    - cron: "0 12 * * *"  # Runs every day at 12 PM UTC

jobs:
  run-scraper:
    runs-on: ubuntu-latest

    steps:
      - name: Checkout repository
        uses: actions/checkout@v3

      - name: Set up Node.js
        uses: actions/setup-node@v3
        with:
          node-version: 18

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.10'

      - name: Install Python dependencies
        run: |
          python -m pip install --upgrade pip
          pip install boto3 requests

      - name: Install Node.js dependencies
        run: npm install

      - name: Run PhantomBuster Agent
        id: run-agent
        run: |
          echo "ðŸŸ¦ Running scrape-and-upload.js..."
          node scrape-and-upload.js > output.log

      - name: Display PhantomBuster script log
        run: cat output.log

      - name: Extract download URLs from log
        id: extract-urls
        run: |
          CSV_URL=$(grep -oP 'https://phantombuster\.s3\.amazonaws\.com[^"]*result\.csv' output.log | tail -1)
          JSON_URL=$(grep -oP 'https://phantombuster\.s3\.amazonaws\.com[^"]*result\.json' output.log | tail -1)
          echo "âœ… CSV_URL: $CSV_URL"
          echo "âœ… JSON_URL: $JSON_URL"
          echo "CSV_URL=$CSV_URL" >> $GITHUB_ENV
          echo "JSON_URL=$JSON_URL" >> $GITHUB_ENV

      - name: Run upload-to-s3.py with extracted URLs
        env:
          ALL_CREDENTIALS: ${{ secrets.ALL_CREDENTIALS }}
          REMOTE_CSV_URL: ${{ env.CSV_URL }}
          REMOTE_JSON_URL: ${{ env.JSON_URL }}
        run: |
          echo "ðŸŸ¨ Using CSV URL: $REMOTE_CSV_URL"
          echo "ðŸŸ¨ Using JSON URL: $REMOTE_JSON_URL"
          python upload-to-s3.py
